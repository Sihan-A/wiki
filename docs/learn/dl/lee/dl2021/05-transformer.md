Transformer
===

![IMG_00001](imgs/05-transformer/IMG_00001.jpg)

Seq2seq
---

![IMG_00002](imgs/05-transformer/IMG_00002.jpg)

![IMG_00003](imgs/05-transformer/IMG_00003.jpg)

![IMG_00004](imgs/05-transformer/IMG_00004.jpg)

![IMG_00005](imgs/05-transformer/IMG_00005.jpg)

![IMG_00006](imgs/05-transformer/IMG_00006.jpg)

### Seq2seq Applications

#### Chatbot

![IMG_00007](imgs/05-transformer/IMG_00007.jpg)

![IMG_00008](imgs/05-transformer/IMG_00008.jpg)

![IMG_00009](imgs/05-transformer/IMG_00009.jpg)

#### Syntactic parsing

==Syntactic parsing== is the <u>automatic analysis of syntactic structure</u> of natural language, especially syntactic relations (in dependency grammar) and labelling spans of constituents (in constituency grammar). (Wikipedia)

- Input: a sentence
- Output: word groups (NP, VP, ADJV, etc)

![IMG_00010](imgs/05-transformer/IMG_00010.jpg)

![IMG_00011](imgs/05-transformer/IMG_00011.jpg)

![IMG_00012](imgs/05-transformer/IMG_00012.jpg)

[arXiv: Grammar as a Foreign Language](https://arxiv.org/abs/1412.7449)

#### Multi-label Classification

![IMG_00013](imgs/05-transformer/IMG_00013.jpg)

#### Object Detection

[arXiv: End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872)

![IMG_00014](imgs/05-transformer/IMG_00014.jpg)

### Seq2seq summary

- [arXiv: Sequence to Sequence Learning with Neural Networks - 2014.09](https://arxiv.org/abs/1409.3215)
- [arXiv: Attention Is All You Need - 2017.06](https://arxiv.org/abs/1706.03762)

![IMG_00015](imgs/05-transformer/IMG_00015.jpg)

Encoder
---

![IMG_00016](imgs/05-transformer/IMG_00016.jpg)

Encoder blocks can be CNN, RNN, or Self-Attention.

![IMG_00017](imgs/05-transformer/IMG_00017.jpg)

![IMG_00018](imgs/05-transformer/IMG_00018.jpg)

![IMG_00019](imgs/05-transformer/IMG_00019.jpg)

![IMG_00020](imgs/05-transformer/IMG_00020.jpg)

![IMG_00021](imgs/05-transformer/IMG_00021.jpg)

Decoder
---

![IMG_00022](imgs/05-transformer/IMG_00022.jpg)

### Autoregressive

![IMG_00023](imgs/05-transformer/IMG_00023.jpg)

![IMG_00024](imgs/05-transformer/IMG_00024.jpg)

![IMG_00025](imgs/05-transformer/IMG_00025.jpg)

![IMG_00026](imgs/05-transformer/IMG_00026.jpg)

Masked multi-head attention

![IMG_00027](imgs/05-transformer/IMG_00027.jpg)

![IMG_00028](imgs/05-transformer/IMG_00028.jpg)

![IMG_00029](imgs/05-transformer/IMG_00029.jpg)

![IMG_00030](imgs/05-transformer/IMG_00030.jpg)

![IMG_00031](imgs/05-transformer/IMG_00031.jpg)

![IMG_00032](imgs/05-transformer/IMG_00032.jpg)

![IMG_00033](imgs/05-transformer/IMG_00033.jpg)

![IMG_00034](imgs/05-transformer/IMG_00034.jpg)

### Non-autoregressive

![IMG_00035](imgs/05-transformer/IMG_00035.jpg)

![IMG_00036](imgs/05-transformer/IMG_00036.jpg)

![IMG_00037](imgs/05-transformer/IMG_00037.jpg)

Encoder-Decoder
---

![IMG_00038](imgs/05-transformer/IMG_00038.jpg)

![IMG_00039](imgs/05-transformer/IMG_00039.jpg)

![IMG_00040](imgs/05-transformer/IMG_00040.jpg)

![IMG_00041](imgs/05-transformer/IMG_00041.jpg)

![IMG_00042](imgs/05-transformer/IMG_00042.jpg)

![IMG_00043](imgs/05-transformer/IMG_00043.jpg)

Training 
---

![IMG_00044](imgs/05-transformer/IMG_00044.jpg)

![IMG_00045](imgs/05-transformer/IMG_00045.jpg)

![IMG_00046](imgs/05-transformer/IMG_00046.jpg)

Tips
---

![IMG_00047](imgs/05-transformer/IMG_00047.jpg)

![IMG_00048](imgs/05-transformer/IMG_00048.jpg)

![IMG_00049](imgs/05-transformer/IMG_00049.jpg)

![IMG_00050](imgs/05-transformer/IMG_00050.jpg)

![IMG_00051](imgs/05-transformer/IMG_00051.jpg)

![IMG_00052](imgs/05-transformer/IMG_00052.jpg)

![IMG_00053](imgs/05-transformer/IMG_00053.jpg)

![IMG_00054](imgs/05-transformer/IMG_00054.jpg)

![IMG_00055](imgs/05-transformer/IMG_00055.jpg)

![IMG_00056](imgs/05-transformer/IMG_00056.jpg)

![IMG_00057](imgs/05-transformer/IMG_00057.jpg)

![IMG_00058](imgs/05-transformer/IMG_00058.jpg)

![IMG_00059](imgs/05-transformer/IMG_00059.jpg)

![IMG_00060](imgs/05-transformer/IMG_00060.jpg)
