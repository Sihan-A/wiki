Self-attention
===

![IMG_00001](imgs/03-attention/IMG_00001.jpg)

Sophisticated Input
---

![IMG_00002](imgs/03-attention/IMG_00002.jpg)

![IMG_00003](imgs/03-attention/IMG_00003.jpg)

![IMG_00004](imgs/03-attention/IMG_00004.jpg)

![IMG_00005](imgs/03-attention/IMG_00005.jpg)

![IMG_00006](imgs/03-attention/IMG_00006.jpg)

![IMG_00007](imgs/03-attention/IMG_00007.jpg)

![IMG_00008](imgs/03-attention/IMG_00008.jpg)

![IMG_00009](imgs/03-attention/IMG_00009.jpg)

![IMG_00010](imgs/03-attention/IMG_00010.jpg)

Self-attention
---

![IMG_00011](imgs/03-attention/IMG_00011.jpg)

![IMG_00012](imgs/03-attention/IMG_00012.jpg)

![IMG_00013](imgs/03-attention/IMG_00013.jpg)

![IMG_00014](imgs/03-attention/IMG_00014.jpg)

![IMG_00015](imgs/03-attention/IMG_00015.jpg)

![IMG_00016](imgs/03-attention/IMG_00016.jpg)

![IMG_00017](imgs/03-attention/IMG_00017.jpg)

![IMG_00018](imgs/03-attention/IMG_00018.jpg)

![IMG_00019](imgs/03-attention/IMG_00019.jpg)

![IMG_00020](imgs/03-attention/IMG_00020.jpg)

![IMG_00021](imgs/03-attention/IMG_00021.jpg)

![IMG_00022](imgs/03-attention/IMG_00022.jpg)

![IMG_00023](imgs/03-attention/IMG_00023.jpg)

![IMG_00024](imgs/03-attention/IMG_00024.jpg)

![IMG_00025](imgs/03-attention/IMG_00025.jpg)

### Multi-head Self-attention

![IMG_00026](imgs/03-attention/IMG_00026.jpg)

![IMG_00027](imgs/03-attention/IMG_00027.jpg)

![IMG_00028](imgs/03-attention/IMG_00028.jpg)

### Positional Encoding

![IMG_00029](imgs/03-attention/IMG_00029.jpg)

![IMG_00030](imgs/03-attention/IMG_00030.jpg)

[arXiv: Learning to Encode Position for Transformer with Continuous Dynamical Model - 2020.03](https://arxiv.org/abs/2003.09229)

Applications of Self-attention
---

![IMG_00031](imgs/03-attention/IMG_00031.jpg)

### Speech 

![IMG_00032](imgs/03-attention/IMG_00032.jpg)

### Image

![IMG_00033](imgs/03-attention/IMG_00033.jpg)

#### GAN & Object Detection 

![IMG_00034](imgs/03-attention/IMG_00034.jpg)

#### Self-attention v.s. CNN

![IMG_00035](imgs/03-attention/IMG_00035.jpg)

![IMG_00036](imgs/03-attention/IMG_00036.jpg)

![IMG_00037](imgs/03-attention/IMG_00037.jpg)

ViT: [arXiv: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - 2020.10](https://arxiv.org/abs/2010.11929)

#### Self-attention v.s. RNN

![IMG_00038](imgs/03-attention/IMG_00038.jpg)

[arXiv: Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention - 2020.06](https://arxiv.org/abs/2006.16236)

![IMG_00039](imgs/03-attention/IMG_00039.jpg)

![IMG_00040](imgs/03-attention/IMG_00040.jpg)

![IMG_00041](imgs/03-attention/IMG_00041.jpg)

![IMG_00042](imgs/03-attention/IMG_00042.jpg)

![IMG_00043](imgs/03-attention/IMG_00043.jpg)